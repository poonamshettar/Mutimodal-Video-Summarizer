{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading pytorch_model.bin: 100%|██████████| 1.63G/1.63G [02:55<00:00, 9.24MB/s]\n",
      "c:\\conda\\envs\\project\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)neration_config.json: 100%|██████████| 363/363 [00:00<00:00, 38.8kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 3.83MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 9.25MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.36MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\'s\n",
    "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
    "# >>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'In recent years, the field of artificial intelligence (AI) has witnessed remarkable growth and innovation. AI, broadly speaking, involves the development of computer systems that can perform tasks that typically require human intelligence. One of the key subsets of AI is machine learning, which focuses on creating algorithms and models.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "ARTICLE = \"\"\" In recent years, the field of artificial intelligence (AI) has witnessed remarkable growth and innovation. AI, broadly speaking, involves the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. One of the key subsets of AI is machine learning, which focuses on creating algorithms and models that allow computers to learn from data, recognize patterns, and make predictions without being explicitly programmed. Machine learning has several techniques, including supervised learning, unsupervised learning, and reinforcement learning.\n",
    "Supervised learning involves training a model on labeled data, where the algorithm learns to map input data to the corresponding correct output. Unsupervised learning, on the other hand, deals with unlabeled data and seeks to find patterns and structure within the data without predefined labels. Reinforcement learning involves an agent learning to make decisions by interacting with an environment, receiving rewards or penalties based on its actions.\n",
    "Deep learning, a subset of machine learning, is particularly powerful and gaining significant traction. It is based on artificial neural networks that have multiple layers (hence the term 'deep'). Deep learning models can automatically discover intricate patterns and features from large amounts of data, making them highly effective in various domains such as image and speech recognition, natural language processing, and autonomous vehicles.\n",
    "The applications of AI and machine learning are vast and impactful. In healthcare, AI is being used for disease diagnosis, drug discovery, and personalized medicine. In finance, machine learning models assist in fraud detection, risk assessment, and investment predictions. Autonomous vehicles utilize AI to enhance safety and enable self-driving capabilities. Natural language processing powers virtual assistants, language translation, and sentiment analysis. Furthermore, AI is continually evolving, with ongoing research and development focusing on improving model efficiency, interpretability, and addressing ethical considerations, including fairness, bias, and privacy in AI systems.\n",
    "\"\"\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))\n",
    "# >>> [{'summary_text': 'Liana Barrientos, 39, is charged with two counts of \"offering a false instrument for filing in the first degree\" In total, she has been married 10 times, with nine of her marriages occurring between 1999 and 2002. She is believed to still be married to four men.'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x=tf.constant([100,200])\n",
    "x.shape\n",
    "x[:,tf.newaxis]\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "y=create_padding_mask(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='The CBI on Saturday booked four former officials of Syndicate Bank and six others for cheating, forgery, criminal conspiracy and causing ₹209 crore loss to the state-run bank. The accused had availed home loans and credit from Syndicate Bank on the basis of forged and fabricated documents. These funds were fraudulently transferred to the companies owned by the accused persons.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "k = [j for i in sentences for j in i.split(' ') if j not in stop_words]\n",
    "sentences = sent_tokenize(' '.join(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['209' 'accused' 'availed' 'bank' 'basis' 'booked' 'causing' 'cbi'\n",
      " 'cheating' 'companies' 'conspiracy' 'credit' 'criminal' 'crore'\n",
      " 'documents' 'fabricated' 'forged' 'forgery' 'former' 'four'\n",
      " 'fraudulently' 'funds' 'home' 'loans' 'loss' 'officials' 'others' 'owned'\n",
      " 'persons' 'run' 'saturday' 'six' 'state' 'syndicate' 'the' 'these'\n",
      " 'transferred']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(sentences)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = vectors.todense()\n",
    "dict={}\n",
    "for i in range(len(sentences)):\n",
    "    dict[sentences[i]]=dense[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = []\n",
    "ave=(sum(dict.values())/len(dict.values()))\n",
    "#print(ave)\n",
    "for i in dict.items():\n",
    "    if i[1]>=ave:\n",
    "        sent.append(i[0])\n",
    "sent=' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CBI Saturday booked four former officials Syndicate Bank six others cheating, forgery, criminal conspiracy causing ₹209 crore loss state-run bank.\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(sent)\n",
    "print(len(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(tf.cast(step,tf.float32))\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "print(learning_rate)\n",
    "print(optimizer.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate Schedule: tf.Tensor(2.1487214e-05, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(tf.cast(step, tf.float32))\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# Instantiate the learning rate schedule\n",
    "learning_rate = CustomSchedule(512)\n",
    "\n",
    "# Call the __call__ method with an integer step value\n",
    "step_value = 123  # Replace with your actual step value\n",
    "lr_value = learning_rate(step_value)\n",
    "\n",
    "print(\"Learning Rate Schedule:\", lr_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           500000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 560501 (2.14 MB)\n",
      "Trainable params: 560501 (2.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"c:\\conda\\envs\\project\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\conda\\envs\\project\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24108\\3783191533.py\", line 43, in <module>\n      model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 2421, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_3463]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\conda\\envs\\project\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'binary_crossentropy/Cast' defined at (most recent call last):\n    File \"c:\\conda\\envs\\project\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\conda\\envs\\project\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\conda\\envs\\project\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24108\\3783191533.py\", line 43, in <module>\n      model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1742, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function\n      return step_function(self, iterator)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n      return self.compiled_loss(\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\conda\\envs\\project\\lib\\site-packages\\keras\\src\\losses.py\", line 2421, in binary_crossentropy\n      y_true = tf.cast(y_true, y_pred.dtype)\nNode: 'binary_crossentropy/Cast'\nCast string to float is not supported\n\t [[{{node binary_crossentropy/Cast}}]] [Op:__inference_train_function_3463]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your dataset from Excel\n",
    "df = pd.read_excel('data\\\\processed.xlsx')\n",
    "num_words = 10000\n",
    "df['Text'] = df['Text'].fillna('')\n",
    "\n",
    "# Convert all values in the \"text\" column to strings\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "# Assuming your Excel file has columns \"text\" and \"label\"\n",
    "X = df['Text'].values\n",
    "y = df['Summary'].values\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_sequence_length = 100\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the LSTM model\n",
    "embedding_dim = 50\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_sequence_length))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

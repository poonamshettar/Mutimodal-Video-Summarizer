{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from transformers import pipeline\n",
    "transcript=YouTubeTranscriptApi.get_transcript('3pmC0SRynFY')\n",
    "transcript_ = ' '.join([d['text'] for d in transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true friend once there were two friends a squirrel and a puppy they used to live and play together the squirrel was very sporty and always won the game [Music] the puppy used to feel bad and thought that it was of no use one day it started raining heavily the squirrel was in high spirits he started doing antics but suddenly lost his balance and fell in the rain water he called his friend the puppy for help the puppy came to his rescue [Music] the squirrel climbed on its back and reached a safe place he thanked his friend for saving his life moral of the story is always have confidence in yourself everyone is unique in his own way thanks for watching [Music] you\n"
     ]
    }
   ],
   "source": [
    "print(transcript_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript='''The true friend. Once there were two friends: a squirrel and a puppy. They used to live and play together. The squirrel was very sporty and always won the game. [Music]. The puppy used to feel bad and thought that it was of no use. One day it started raining heavily. The squirrel was in high spirits; he started doing antics but suddenly lost his balance and fell into the rainwater. He called his friend, the puppy, for help. [Music]. The puppy came to his rescue. The squirrel climbed on its back and reached a safe place. He thanked his friend for saving his life. Moral of the story is always to have confidence in yourself; everyone is unique in their own way. Thanks for watching. [Music]. You.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using transformer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Your max_length is set to 142, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n"
     ]
    }
   ],
   "source": [
    "summariser = pipeline('summarization')\n",
    "summary = ''\n",
    "for i in range(0, (len(transcript_)//1000)+1):\n",
    "        summary_text = summariser(transcript_[i*1000:(i+1)*1000])[0]['summary_text']\n",
    "        summary = summary + summary_text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I give a look-alike photo of my favorite hero or cricket star on my facebook so you could keep a photo . People say hey you look like Deepika Padukone, I want to be looked like Katrina Katrina, I. looked like Dhoni, I . want to look like Deepika Padukone and Katrina Katrina Katrina . But when you are not being identified as you as somebody else then you are feeling so proud in that and then you . say nobody takes me seriously why will they take you seriously when you . are not proud about being not proud .  Your individuality that you are somebody and your opinions matter and for that a deep thinking has to happen. reflection has to be part of your routine.   your individuality that  you are  somebody  and  opinions matter . For that, a deep reflection must happen. contemplation has to become part of the routine. \n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-Lsum Precision: 0.584070796460177\n",
      "Rouge-Lsum Recall: 0.8571428571428571\n",
      "Rouge-Lsum F1 Score: 0.6947368372847645\n"
     ]
    }
   ],
   "source": [
    "def evaluation2():\n",
    "    from rouge import Rouge\n",
    "    rouge=Rouge()\n",
    "    scores=rouge.get_scores(transcript_,summary,avg=True)\n",
    "    print(\"Rouge-Lsum Precision:\", scores[\"rouge-l\"][\"p\"])\n",
    "    print(\"Rouge-Lsum Recall:\", scores[\"rouge-l\"][\"r\"])\n",
    "    print(\"Rouge-Lsum F1 Score:\", scores[\"rouge-l\"][\"f\"])\n",
    "evaluation2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1: Score(precision=0.8707482993197279, recall=0.5925925925925926, fmeasure=0.7052341597796142)\n",
      "rouge2: Score(precision=0.773972602739726, recall=0.5255813953488372, fmeasure=0.6260387811634348)\n",
      "rougeL: Score(precision=0.673469387755102, recall=0.4583333333333333, fmeasure=0.5454545454545454)\n"
     ]
    }
   ],
   "source": [
    "def evaluation1():\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(transcript_,summary)\n",
    "    for key in scores:\n",
    "        print(f'{key}: {scores[key]}')\n",
    "evaluation1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio for the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "text_speech = pyttsx3.init()\n",
    "current_rate = text_speech.getProperty('rate')\n",
    "new_rate = 150\n",
    "text_speech.setProperty('rate', new_rate)\n",
    "text_speech.say(summary)\n",
    "text_speech.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\project\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "from transformers import PegasusTokenizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/pegasus-xsum\"\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Create tokens\n",
    "tokens = pegasus_tokenizer(summary, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
    "encoded_summary = pegasus_model.generate(**tokens)\n",
    "\n",
    "# Decode summarized text\n",
    "decoded_summary = pegasus_tokenizer.decode(\n",
    "      encoded_summary[0],\n",
    "      skip_special_tokens=True\n",
    ")\n",
    "summarizer = pipeline(\n",
    "    \"summarization\", \n",
    "    model=model_name, \n",
    "    tokenizer=pegasus_tokenizer, \n",
    "    framework=\"pt\"\n",
    ")\n",
    "summary = summarizer(summary, min_length=30, max_length=150)\n",
    "summary[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can choose other GPT-2 variants like \"gpt2-medium,\" \"gpt2-large,\" etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Input text to be summarized\n",
    "input_text = \"\"\"\n",
    "Your input text goes here. This is the text you want to generate a summary for.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "# Generate the summary\n",
    "summary_ids = model.generate(input_ids, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the generated summary back to text\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the generated summary\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
